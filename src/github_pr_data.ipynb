{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull request data from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "from dateutil.parser import parse as parse_date\n",
    "import pandas as pd\n",
    "from config import github_pat as pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Code With Your Parameters\n",
    "\n",
    "# owner and repo_names are used to construct the urls for the github api to access repo data\n",
    "owner = '<the github org>'\n",
    "team_slug = '<the github team name>'\n",
    "\n",
    "repo_names = [\n",
    "  'repo1',\n",
    "  'repo2',\n",
    "  '...',\n",
    "  ]\n",
    "\n",
    "# the date range you're interested in\n",
    "start_date = '2023-08-01T00:00:00Z'\n",
    "end_date = '2023-10-31T23:59:59Z'\n",
    "start_week = parse_date(start_date).isocalendar().week\n",
    "end_week = parse_date(end_date).isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get team members\n",
    "# -- this assumes there is less than 1 page of team members (100) --\n",
    "\n",
    "headers = {\n",
    "  'Accept': 'application/vnd.github+json',\n",
    "  'Authorization': 'Bearer ' + pat,\n",
    "  'X-GitHub-Api-Version': '2022-11-28'\n",
    "  }\n",
    "\n",
    "url = f'https://api.github.com/orgs/{owner}/teams/{team_slug}/members'\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "if (r.status_code != requests.codes.ok):\n",
    "  r.raise_for_status()\n",
    "\n",
    "gh_account_list = [ user['login'] for user in r.json()]\n",
    "\n",
    "print(gh_account_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make requests\n",
    "per_page = 30\n",
    "params = {\n",
    "  'state': 'closed',\n",
    "  'page': 1,\n",
    "  'per_page': per_page,\n",
    "  }\n",
    "\n",
    "headers = {\n",
    "  'Accept': 'application/vnd.github+json',\n",
    "  'Authorization': 'Bearer ' + pat,\n",
    "  'X-GitHub-Api-Version': '2022-11-28'}\n",
    "\n",
    "pr_data_dict = dict()\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  print('fetching '+repo_name+'...')\n",
    "  page = 1\n",
    "  url = 'https://api.github.com/repos/' + str(owner) + '/' + str(repo_name) + '/pulls' \n",
    "  pr_data_dict[repo_name] = []\n",
    "\n",
    "  # Paginate. This might break given edge cases\n",
    "  while(True):\n",
    "    print('page '+str(page))\n",
    "    params['page'] = page\n",
    "    r = requests.get(url, headers=headers, params=params)\n",
    "    pr_data_dict[repo_name].extend(r.json())\n",
    "    if parse_date(pr_data_dict[repo_name][-1]['created_at']) >= parse_date(start_date) and len(r.json()) >= per_page:\n",
    "      page += 1\n",
    "      continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and clean data we want from each PR\n",
    "\n",
    "# You can access any nested properties using '.' notation in the data_fields array, \n",
    "# such as user.login. This should work for an arbitrary depth of data\n",
    "data_fields = ['id', 'number', 'created_at', 'merged_at', 'user.login']\n",
    "\n",
    "def recursive_extract_nested_data(data_dict, complex_key):\n",
    "  keys = complex_key.rsplit('.')\n",
    "  if len(keys) == 1:\n",
    "    if (keys[0] == 'created_at' or keys[0] == 'merged_at'):\n",
    "      try:\n",
    "        return parse_date(data_dict[keys[0]])\n",
    "      except:\n",
    "        return data_dict[keys[0]]\n",
    "    return data_dict[keys[0]]\n",
    "  return recursive_extract_nested_data(data_dict[keys[0]], '.'.join(keys[1:]))\n",
    "\n",
    "def truncate_pr_data(pr_data_dict, data_fields):\n",
    "  truncated_pr_data_list = []\n",
    "  for pr_data in pr_data_dict:\n",
    "    truncated_pr_data = {}\n",
    "    for data_field in data_fields:\n",
    "      truncated_pr_data[data_field] = recursive_extract_nested_data(pr_data, data_field)\n",
    "    truncated_pr_data_list.append(truncated_pr_data)\n",
    "  \n",
    "  return truncated_pr_data_list\n",
    "\n",
    "cleaned_pr_data = {}\n",
    "for repo_name in repo_names:\n",
    "  cleaned_pr_data[repo_name] = truncate_pr_data(pr_data_dict[repo_name], data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframes in time range\n",
    "\n",
    "data_frames = {}\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  data_frames[repo_name] = pd.DataFrame(cleaned_pr_data[repo_name])\n",
    "  data_frames[repo_name] = data_frames[repo_name].loc[(data_frames[repo_name].created_at >= start_date) & (data_frames[repo_name].created_at < end_date)]\n",
    "\n",
    "data_frames[repo_names[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weekly PR data\n",
    "\n",
    "weeks_data = {} \n",
    "week_groups_df = pd.DataFrame()\n",
    "\n",
    "start_week = parse_date(start_date).isocalendar().week\n",
    "end_week = parse_date(end_date).isocalendar().week\n",
    "\n",
    "# Copy df into weeks\n",
    "for repo_name in repo_names:\n",
    "  weeks = pd.DataFrame(data_frames[repo_name])\n",
    "  weeks['created_at'] = weeks['created_at'].dt.isocalendar().week\n",
    "\n",
    "  weeks_data[repo_name] = weeks\n",
    "  # TODO Figure out why it's dropping weeks 36 & 37...\n",
    "#   week_groups_df[repo_name] = weeks.groupby(weeks['created_at'])['id'].count()\n",
    "\n",
    "# week_groups_df.plot(kind='bar', figsize=(15,7), xlabel='PR Creation Week', ylabel='Count', title='PR Creations per Week by Service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total PRs by repo\n",
    "\n",
    "pr_counts = {}\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  pr_counts[repo_name] = data_frames[repo_name]['id'].count()\n",
    "\n",
    "pr_counts_df = pd.DataFrame.from_dict(pr_counts, orient='index')\n",
    "\n",
    "pr_counts_df.plot(kind='bar', title=\"Total PRs by Repo between week \"+str(start_week)+\" and \"+str(end_week), xlabel=\"Repo Name\", ylabel='Count', legend=None, grid=True, figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total PRs by repo by team/non-team\n",
    "\n",
    "pr_counts_with_team = {}\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  pr_counts_with_team[repo_name] = {}\n",
    "  on_team = data_frames[repo_name][data_frames[repo_name]['user.login'].isin(gh_account_list)]\n",
    "  not_on_team = data_frames[repo_name][~data_frames[repo_name]['user.login'].isin(gh_account_list)]\n",
    "\n",
    "  pr_counts_with_team[repo_name]['on_team'] = on_team['user.login'].count()\n",
    "  pr_counts_with_team[repo_name]['not_on_team'] = not_on_team['user.login'].count()\n",
    "  \n",
    "\n",
    "pr_counts_with_team_df = pd.DataFrame.from_dict(pr_counts_with_team, orient='columns').T\n",
    "print(pr_counts_with_team_df)\n",
    "\n",
    "pr_counts_with_team_df.plot.bar(stacked=True, title=\"Total PRs by Repo between week \"+str(start_week)+\" and \"+str(end_week), xlabel=\"Repo Name\", ylabel='Count', grid=True, figsize=(15,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
